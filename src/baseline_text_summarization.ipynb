{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install rouge","execution_count":null,"outputs":[]},{"metadata":{"id":"JuXNMv875aKj","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function, division\n%matplotlib inline\ndebug = True\nto_print_epoch = True\nfrom nltk.translate import bleu\nfrom nltk.corpus import stopwords\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\n\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport random\nimport os\n\n# Pytorch Functionlaities\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport time\nimport math\nfrom rouge import Rouge\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"id":"aZ4wR517J5Ge","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 200\nroot_directory = \"/kaggle/input/google-dataset/\"\nepoch = 1000\nprint_every_epoch = 5\nno_of_hidden_size = 512\nlcl_learning_rate = 0.01\ndropout = 0\nn_layers = 2\nteacher_forcing_ratio = 0.5","execution_count":null,"outputs":[]},{"metadata":{"id":"GN-mBamK4fxK","colab_type":"text"},"cell_type":"markdown","source":"Prepare Input Data For Training "},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\"didn't\": \"did not\",\n\"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\"you're\": \"you are\", \"you've\": \"you have\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_text(text,flag=False):\n    stop_words = stopwords.words('english')\n    text = text.lower()\n    text = text.replace('\\n','')\n    text = re.sub(r'\\(.*\\)','',text)\n    text = re.sub(r'[^a-zA-Z0-9. ]','',text)\n    text = re.sub(r'\\.',' . ',text)\n    text = text.replace('.','')\n    text = text.split()\n    for i in range(len(text)):\n        word = text[i]\n        if word in contraction_mapping:\n            text[i] = contraction_mapping[word]\n    newtext = []\n    for word in text:\n        if word not in stop_words and len(word)>0:\n            newtext.append(word)\n    text = newtext\n    if flag:\n        text = text[::-1]\n    text = \" \".join(text)\n    text = text.replace(\"'s\",'') \n    return text","execution_count":null,"outputs":[]},{"metadata":{"id":"fruhDAYW4i83","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def prepareInput(sourcefilePath,destFilePath,fileName):\n    source = sourcefilePath\n    target = destFilePath\n    save_trans = fileName\n\n    corpus_source = open(source, 'r').readlines()\n    corpus_target = open(target, 'r').readlines()\n  \n    writer = open(save_trans, 'w')\n    for k, v in zip(corpus_source, corpus_target):\n        k = process_text(k,True)\n        v = process_text(v,False)\n        writer.write(k + '\\t' + v + '\\n')\n    writer.flush()\n    writer.close()","execution_count":null,"outputs":[]},{"metadata":{"id":"Unn3Qxoc5mH7","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"SOS_token = 0\nEOS_token = 1\nUNK_token = 2\nPAD_token = 3\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name  \n        self.word2index,self.word2count,self.index2word = {},{},{}\n        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token:\"UNK\"}\n        self.n_words = 3  # Count SOS and EOS and UNK\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","execution_count":null,"outputs":[]},{"metadata":{"id":"nFdXDjMs6LRu","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def parseLanguageInput(lang1,lang2,fullFilePath):\n\n    lines = open(fullFilePath, encoding='utf-8').read().strip().split('\\n')\n    pairs = [[s for s in l.split('\\t')] for l in lines]\n\n    input_lang = Lang(lang1)\n    output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs\n\ndef prepareData(lang1,lang2,fullFilePath):\n    input_lang, output_lang, pairs = None,None,None\n    input_lang, output_lang, pairs = parseLanguageInput(lang1,lang2,fullFilePath)\n\n    for pair in pairs:\n      input_lang.addSentence(pair[0])\n      output_lang.addSentence(pair[1])\n\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n\n    return input_lang, output_lang, pairs","execution_count":null,"outputs":[]},{"metadata":{"id":"FA8iHwMbEYNI","colab_type":"text"},"cell_type":"markdown","source":"Designing Encoder"},{"metadata":{"id":"_ayghPqrEc7K","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, num_layers = n_layers, bidirectional=True)\n\n    def forward(self, input, hidden):\n#         print(input.size())\n#         print(hidden.size())\n        embedded = self.embedding(input).view(1, 1, -1)\n#         print(embedded.size())\n        output = embedded#.unsqueeze(0)\n#         print(output.size())\n        \n        output, hidden = self.gru(output, hidden)\n#         print(\"Encoder: \",output.size(),hidden.size())\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(n_layers*2, 1, self.hidden_size, device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size , num_layers = n_layers, bidirectional=True)\n        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n#         print(\"Decoder: \",attn_weights.size(),attn_weights.unsqueeze(0).size() )\n#         print(\"Decoder: \",encoder_outputs.size(), encoder_outputs.unsqueeze(0).size())\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n#         print(\"Decoder: \",attn_applied.size())\n#         print(\"Decoder: \",embedded.size())\n#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = attn_applied[0]\n        \n#         print(\"Decoder: \",output.size())\n        output = self.attn_combine(output)\n#         print(\"Decoder: \",output.size())\n        output = output.unsqueeze(0)\n#         print(\"Decoder: \",output.size())\n        output = F.relu(output)\n#         print(\"Decoder: \",output.size())\n#         print(\"Decoder: \",hidden.size())\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(2, 1, self.hidden_size, device=device)","execution_count":null,"outputs":[]},{"metadata":{"id":"ANz4R2NQF98m","colab_type":"text"},"cell_type":"markdown","source":"Helper Methods -----\nPrepare Training Data to put EndOfMarker at end of the Sentence"},{"metadata":{"id":"7mnnYIF5Fu08","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] if word in lang.word2index else UNK_token for word in sentence.split(' ')]\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train"},{"metadata":{"id":"8YDXZRgXGG2t","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n\n    loss = 0\n    \n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length","execution_count":null,"outputs":[]},{"metadata":{"id":"-LKWqUBnHsP4","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n    start = time.time()\n    plot_losses = []\n    no_of_epoch = []\n    print_loss_total = 0 \n    plot_loss_total = 0\n\n    encoder_optimizer = optim.ASGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.ASGD(decoder.parameters(), lr=learning_rate)\n    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, n_iters + 1):\n        \n        training_pair = training_pairs[iter - 1]\n        input_tensor,target_tensor = training_pair[0],training_pair[1]\n        \n        loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)\n\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total / print_every\n            print_loss_total = 0\n            if to_print_epoch:\n                  print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))\n\n        if iter % plot_every == 0:\n            plot_loss_avg = plot_loss_total / plot_every\n            plot_losses.append(plot_loss_avg)\n            no_of_epoch.append(iter)\n            plot_loss_total = 0\n\n    return plot_losses,no_of_epoch    ","execution_count":null,"outputs":[]},{"metadata":{"id":"7GBOQ-PwIKwg","colab_type":"text"},"cell_type":"markdown","source":"Evaluate the Model"},{"metadata":{"id":"2y9lWyBYIEwX","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    with torch.no_grad():\n#         print(sentence)\n        input_tensor = tensorFromSentence(input_lang, sentence)\n#         print(input_tensor.size())\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n            encoder_outputs[ei] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n\n        decoder_hidden = encoder_hidden\n\n        decoded_words = []\n        decoder_attentions = torch.zeros(max_length, max_length)\n\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(output_lang.index2word[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words, decoder_attentions[:di + 1]","execution_count":null,"outputs":[]},{"metadata":{"id":"zwJEALNJJC25","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"prefix = \"train\"\nsourceLangPath = root_directory +prefix+\".original\"\nsourcePrefix = \"original\"\ntargetLangPath = root_directory+prefix+\".compressed\"\ntargetPrefix = \"compressed\"\nfullFilePathForData = prefix + \"_\" + sourcePrefix + \"_\" + targetPrefix + \".txt\"\nteacher_forcing_ratio = 0.5","execution_count":null,"outputs":[]},{"metadata":{"id":"2EOi15XpIhTM","colab_type":"text"},"cell_type":"markdown","source":"Set Up the Input Data "},{"metadata":{"id":"hzGuU5b6If6z","colab_type":"code","outputId":"c2d27748-3f1c-465b-fd34-18df1af3e145","colab":{"base_uri":"https://localhost:8080/","height":104},"trusted":true},"cell_type":"code","source":"# create Custom File having both Langauges  \nisFilePresent = os.path.isfile(fullFilePathForData)\n# if isFilePresent == False:\nprint(\"New File Created\")\n# make_connection()\nprepareInput(sourceLangPath,targetLangPath,fullFilePathForData)\n# else:\n#   print(\"File is not created Again\")\n\ninput_lang, output_lang, pairs = prepareData(sourcePrefix, targetPrefix,fullFilePathForData)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"lpw7oO0eeB-4","colab_type":"code","outputId":"06fbd8ba-c429-4c79-afe0-e611c25fe780","colab":{"base_uri":"https://localhost:8080/","height":89},"trusted":true},"cell_type":"code","source":"print(pairs[0])\nprint(pairs[1])\nprint(pairs[2])","execution_count":null,"outputs":[]},{"metadata":{"id":"6dmCkESMBYiT","colab_type":"code","outputId":"0f69dbd9-48ad-4c57-ad68-75302b44a47e","colab":{"base_uri":"https://localhost:8080/","height":139},"trusted":true},"cell_type":"code","source":"def load_saved_encoder(lcl_input_lang,lcl_hidden_size,lcl_encoder_model_path):\n    device = torch.device('cpu')\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    encoder_model = EncoderRNN(lcl_input_lang.n_words, lcl_hidden_size).to(device)\n    encoder_model.load_state_dict(torch.load(lcl_encoder_model_path, map_location=device))\n    return encoder_model\n\ndef load_saved_decoder(lcl_hidden_size,lcl_output_lang,lcl_decoder_model_path):\n    # device = torch.device('cpu')\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    decoder_model = AttnDecoderRNN(lcl_hidden_size,lcl_output_lang.n_words,dropout,MAX_LENGTH).to(device)\n    decoder_model.load_state_dict(torch.load(lcl_decoder_model_path, map_location=device))\n    return decoder_model\n\ndef load_obj(obj_type,obj_name_path):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    obj_type = torch.load(obj_name_path, map_location=device)\n    return obj_type\n  \nencoder_model_path = \"Encoder_Model.pt\"\ndecoder_model_path = \"Decoder_Model.pt\"\nparams = \"params.pt\"\nvocab_params = \"vocab.pt\"\n\ntrain_result_data_path = \"train_Result_Epoch_\"+ str(epoch) +\"_Hid_size_\"+ str(no_of_hidden_size) +\".pt\"\ntest_result_data_path = \"test_Result_Epoch_\"+ str(epoch) +\"_Hid_size_\"+ str(no_of_hidden_size) + \".pt\"   \nencoder_model_path = \"Ep_\"+ str(epoch) +\"_Hd_\"+ str(no_of_hidden_size) + \"_lr_\"+ str(lcl_learning_rate) +\"_\"+ encoder_model_path   \ndecoder_model_path = \"Ep_\"+ str(epoch) +\"_Hd_\"+ str(no_of_hidden_size) + \"_lr_\"+ str(lcl_learning_rate) +\"_\"+ decoder_model_path \nvocab_params = \"Ep_\"+ str(epoch) +\"_Hd_\"+ str(no_of_hidden_size) + \"_lr_\"+ str(lcl_learning_rate) +\"_\"+ vocab_params \n\nprint(\"Encoder Model Path :\" ,encoder_model_path)\nprint(\"Decoder Model Path :\" ,decoder_model_path)\nprint(\"params Path :\" ,params)\nprint(\"train file Path :\",train_result_data_path)\nprint(\"test file Path :\",test_result_data_path)\nprint(\"Encoder Model Exist \" ,os.path.isfile(encoder_model_path))\nprint(\"Decoder Model Exist \" ,os.path.isfile(decoder_model_path))","execution_count":null,"outputs":[]},{"metadata":{"id":"6IOuK4yVIkun","colab_type":"text"},"cell_type":"markdown","source":"Invoke Seq2Seq Model"},{"metadata":{"id":"jeZJdDZKITrX","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"encoder_model = None\ndecoder_model = None\nmodel_performance = {}\n\n# if os.path.isfile(encoder_model_path) == False or os.path.isfile(decoder_model_path) == False:\nhidden_size = no_of_hidden_size\nencoder_model = EncoderRNN(input_lang.n_words, hidden_size).to(device)\ndecoder_model = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=dropout).to(device)\nplot_losses,no_of_epoch = trainIters(encoder_model, decoder_model, epoch, print_every=print_every_epoch,learning_rate=lcl_learning_rate)\nfolder_path = F\"\"+root_directory+\"{encoder_model_path}\"\ntorch.save(encoder_model.state_dict(), encoder_model_path)\nfolder_path = F\"\"+root_directory+\"{decoder_model_path}\"\ntorch.save(decoder_model.state_dict(), decoder_model_path)\n\nlcl_params = {'loss_list':plot_losses,'epoch_list':no_of_epoch,'lr':lcl_learning_rate,\"dropout\":dropout, \"MAX_LENGTH\":MAX_LENGTH,\"epoch\":epoch, \"no_of_hidden_size\":no_of_hidden_size}\nlcl_params[\"lcl_learning_rate\"] = lcl_learning_rate\n\nvocab = {'input_lang':input_lang, 'output_lang':output_lang}\nmodel_performance = lcl_params\ntorch.save(lcl_params, params)\ntorch.save(vocab, vocab_params)\n# else:\n#     print(\"Model Already Exist\")\n#     hidden_size = no_of_hidden_size\n#     encoder_model = load_saved_encoder(input_lang,hidden_size,encoder_model_path)\n#     decoder_model = load_saved_decoder(hidden_size,output_lang,decoder_model_path)\n#     model_performance = load_obj(model_performance,params)\n#     vocab = load_obj(vocab,vocab_params)\nprint(model_performance)\nprint(vocab)\nprint(encoder_model)\nprint(decoder_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(plot_losses)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ccXOLinaJbdq","colab_type":"text"},"cell_type":"markdown","source":"Assessment of Model"},{"metadata":{"id":"4Hz43TLWJNce","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction    \n\ndef calculate_rouge(rouge, pred_trg, real_trg):\n\n    pred_trg = \" \".join(pred_trg)\n    real_trg = \" \".join(real_trg[0])\n    if len(pred_trg) > len(real_trg):\n        diff = len(pred_trg) - len(real_trg)\n        real_trg = real_trg +\" \"+  \"#\"*(diff-1)\n    elif len(pred_trg) < len(real_trg):\n        diff = len(real_trg) - len(pred_trg)\n        pred_trg = pred_trg +\" \"+ \"#\"*(diff-1)\n    scores = rouge.get_scores(pred_trg, real_trg)\n    return scores \n\ndef calculate_Result(encoder, decoder,lcl_pairs, n=50):\n    result_value_rouge_score = []\n    rouge = Rouge()\n    \n    for i in range(n):\n        pair = random.choice(lcl_pairs)\n\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n\n        reference = [pair[1].split()]\n\n        output_words = output_words[:-1]\n        target_predicted = output_words\n        \n        score = calculate_rouge(rouge,target_predicted,reference)\n        result_value_rouge_score.append((pair[0],pair[1].split(),target_predicted,score))\n\n    return result_value_rouge_score","execution_count":null,"outputs":[]},{"metadata":{"id":"uNncrRLYhIXB","colab_type":"text"},"cell_type":"markdown","source":"Testing on Trained data"},{"metadata":{"id":"8V9mBXukVllW","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"result_value_rouge_score = calculate_Result(encoder_model, decoder_model,pairs)\nresult_value_rouge_score_dict = {}\nresult_value_rouge_score_dict['result'] = result_value_rouge_score \ntorch.save(result_value_rouge_score_dict, train_result_data_path)\nfor item in result_value_rouge_score:\n  print(\" Source Language \",item[0])\n  print(\" Input Target\",item[1])\n  print(\" Output Target\",item[2])\n  print(\" Score \",item[3])","execution_count":null,"outputs":[]},{"metadata":{"id":"hFzAZ-5oYISA","colab_type":"text"},"cell_type":"markdown","source":"Testing on Test Data"},{"metadata":{"id":"zrDpju7MYPdW","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"prefix = \"test\"\nsourceLangPath = root_directory +prefix+\".original\"\nsourcePrefix = \"original\"\ntargetLangPath = root_directory+prefix+\".compressed\"\ntargetPrefix = \"compressed\"\nfullFilePathForData = prefix + \"_\" + sourcePrefix + \"_\" + targetPrefix + \".txt\"","execution_count":null,"outputs":[]},{"metadata":{"id":"g2lUbG-gVuC3","colab_type":"code","outputId":"426f3794-180d-4205-b892-1b32f9410a79","colab":{"base_uri":"https://localhost:8080/","height":139},"trusted":true},"cell_type":"code","source":"# create Custom File having both Langauges  \nisFilePresent = os.path.isfile(fullFilePathForData)\n# if isFilePresent == False:\nprint(\"New File Created\")\n# make_connection()\nprepareInput(sourceLangPath,targetLangPath,fullFilePathForData)\n# else:\n#   print(\"File is not created Again\")\n\ntest_input_lang, test_output_lang, test_pairs = prepareData(sourcePrefix, targetPrefix,fullFilePathForData)\nprint(random.choice(test_pairs))","execution_count":null,"outputs":[]},{"metadata":{"id":"r9cGzi7pY9U6","colab_type":"code","outputId":"67e744d4-4689-4a9f-dbb9-b03ca3e8c4d6","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"result_value_rouge_score_test = calculate_Result(encoder_model, decoder_model,test_pairs)\nresult_value_rouge_score_dict_test = {}\nresult_value_rouge_score_dict_test['result'] = result_value_rouge_score_test \ntorch.save(result_value_rouge_score_dict_test, test_result_data_path)\nfor item in result_value_rouge_score_test:\n    print(\" Source Language \",item[0])\n    print(\" Input Target\",item[1])\n    print(\" Output Target\",item[2])\n    print(\" Score \",item[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}